[id="cnf-best-practices-openshift-operations"]
= Operations that shall be executed by OpenShift

The application should not require `NET_ADMIN` capability to perform the following administrative operations:

[id="cnf-best-practices-mtu-setting"]
== Setting the MTU

* Configure the MTU for the cluster network, also known as the OVN or Openshift-SDN network, by modifying the manifests generated by `openshift-installer` before deploying the cluster. See link:https://docs.openshift.com/container-platform/latest/networking/changing-cluster-network-mtu.html[Changing the MTU for the cluster network] for more information.

* Configure additional networks managed by the Cluster Network Operator by using `NetworkAttachmentDefinition` resources generated by the Cluster Network Operator. See link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html[Using high performance multicast] for more information.

* Configure SR-IOV interfaces by using the SR-IOV Network Operator, see link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/configuring-sriov-device.html[Configuring an SR-IOV network device] for more information.

[id="cnf-best-practices-link-state-modification"]
== Modifying link state

* All the links should be set up before attaching it to a pod.

[id="cnf-best-practices-ip/mac-address-assignment"]
== Assigning IP/MAC addresses

* For all the networks, the IP/MAC address should be assigned to the interface during pod creation.

* MULTUS also allows users to override the IP/MAC address. Refer to link:https://docs.openshift.com/container-platform/latest/networking/multiple_networks/attaching-pod.html[Attaching a pod to an additional network] for more information.

[id="cnf-best-practices-manipulate-pod’s-route-table"]
== Manipulating pod route tables

* By default, the default route of the pod will point to the cluster network, with or without the additional networks. MULTUS also allows users to override the default route of the pod. Refer to link:https://docs.openshift.com/container-platform/latest/networking/multiple_networks/attaching-pod.html[Attaching a pod to an additional network] for more information.

* Non-default routes can be added to pod routing tables by various IPAM CNI plugins during pod creation.

[id="cnf-best-practices-sr/iov-vf-setting"]
== Setting SR/IOV VFs

The SR-IOV Network Operator also supports configuring the following parameters for SR-IOV VFs. Refer to link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/configuring-sriov-net-attach.html[Configuring an SR-IOV Ethernet network attachment] for more information.

* `vlan`
* `linkState`
* `maxTxRate`
* `minRxRate`
* `vlanQoS`
* `spoofChk`
* `trust`

[id="cnf-best-practices-multicast"]
== Configuring multicast

In OpenShift, multicast is supported for both the default interface (OVN or OpenShift-SDN) and the additional interfaces such as macvlan, SR-IOV, etc. Multicast is disabled by default. To enable it, refer to the following procedures:

* link:https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/enabling-multicast.html[Enabling multicast for a project]
* link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html#nw-using-an-sriov-interface-for-multicast_using-sriov-multicast[Configuring an SR-IOV interface for multicast]
* If your application works as a multicast source and you want to utilize the additional interfaces to carry the multicast traffic, then you don’t need the `NET_ADMIN` capability. Follow the instructions in link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html[Using high performance multicast] to set the correct multicast route in the pod’s routing table.

[id="cnf-best-practices-operations-that-can-not-be-executed-by-openshift"]
== Operations that can not be executed by OpenShift

All the CNI plugins are only invoked during pod creation and deletion. If your CNF needs perform any operations mentioned above at runtime, the `NET_ADMIN` capability is required.

There are some other functionalities that are not currently supported by any of the OpenShift components which also require `NET_ADMIN` capability:

* Link state modification at runtime

* IP/MAC modification at runtime

* Manipulate pod’s route table or firewall rules at runtime

* SR/IOV VF setting at runtime

* Netlink configuration

* For example, `ethtool` can be used to configure things like rxvlan, txvlan, gso, tso, etc.

* Multicast
+
[NOTE]
====
If your application works as a receiving member of IGMP groups, you need to specify the NET_ADMIN capability in the pod manifest. So that the app is allowed to assign multicast addresses to the pod interface and join an IGMP group.
====

* Set `SO_PRIORITY` to a socket to manipulate the 802.1p priority in ethernet frames

* Set `IP_TOS` to a socket to manipulate the DSCP value of IP packets

[id="cnf-best-practices-analyzing-your-application"]
== Analyzing your application

To find out which capabilities the application needs, Red Hat has developed a SystemTap script (`container_check.stp`). With this tool, the CNF developer can find out what capabilities an application requires in order to run in a container. It also shows the syscalls which were invoked. Find more info at link:https://linuxera.org/capabilities-seccomp-kubernetes/[]

Another tool is `capable` which is part of the BCC tools. It can be installed on RHEL8 with `dnf install bcc`.

[id="cnf-best-practices-example"]
=== Finding the capabilities that an application needs

Here is an example of how to find out the capabilities that an application needs. `testpmd` is a DPDK based layer-2 forwarding application. It needs the `CAP_IPC_LOCK` to allocate the hugepage memory.

. Use container_check.stp. We can see `CAP_IPC_LOCK` and `CAP_SYS_RAWIO` are requested by `testpmd` and the relevant syscalls.
+
[source,terminal]
----
$ $ /usr/share/systemtap/examples/profiling/container_check.stp -c 'testpmd -l 1-2 -w 0000:00:09.0 -- -a --portmask=0x8 --nb-cores=1'
----
+
.Example output
[source,terminal]
----
[...]
capabilities used by executables
    executable:   prob capability
    testpmd:      cap_ipc_lock
    testpmd:      cap_sys_rawio

capabilities used by syscalls
    executable,   syscall ( capability )    : count
    testpmd,      mlockall ( cap_ipc_lock ) : 1
    testpmd,      mmap ( cap_ipc_lock )     : 710
    testpmd,      open ( cap_sys_rawio )    : 1
    testpmd,      iopl ( cap_sys_rawio )    : 1

failed syscalls
    executable,          syscall =       errno:   count
    eal-intr-thread,  epoll_wait =       EINTR:       1
    lcore-slave-2,          read =            :       1
    rte_mp_handle,       recvmsg =            :       1
    stapio,                      =       EINTR:       1
    stapio,               execve =      ENOENT:       3
    stapio,        rt_sigsuspend =            :       1
    testpmd,               flock =      EAGAIN:       5
    testpmd,                stat =      ENOENT:      10
    testpmd,               mkdir =      EEXIST:       2
    testpmd,            readlink =      ENOENT:       3
    testpmd,              access =      ENOENT:    1141
    testpmd,              openat =      ENOENT:       1
    testpmd,                open =      ENOENT:      13
    [...]
----

. Use the `capable` command:
+
[source,terminal]
----
$ /usr/share/bcc/tools/capable
----

. Start the testpmd application from another terminal, and send some test traffic to it. For example:
+
[source,terminal]
----
$ testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----

. Check the output of the `capable` command. Below, `CAP_IPC_LOCK` was requested for running `testpmd`.
+
[source,terminal]
----
[...]
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
0:41:58 0 3591  testpmd CAP_IPC_LOCK  1
[...]
----

. Also, try to run `testpmd` without `CAP_IPC_LOCK` set with `capsh`. Now we can see that the hugepage memory cannot be allocated.

[source,terminal]
----
$ capsh --drop=cap_ipc_lock -- -c testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----
+
.Example output
[source,terminal]
----
EAL: Detected 24 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: No free hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: VFIO support initialized
EAL: PCI device 0000:01:00.0 on NUMA socket 0
EAL: probe driver: 8086:10fb net_ixgbe
EAL: using IOMMU type 1 (Type 1)
EAL: Ignore mapping IO port bar(2)
EAL: PCI device 0000:01:00.1 on NUMA socket 0
EAL: probe driver: 8086:10fb net_ixgbe
EAL: PCI device 0000:07:00.0 on NUMA socket 0
EAL: probe driver: 8086:1521 net_e1000_igb
EAL: PCI device 0000:07:00.1 on NUMA socket 0
EAL: probe driver: 8086:1521 net_e1000_igb
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: mlockall() failed with error "Cannot allocate memory" testpmd: create a new mbuf pool <mbuf_pool_socket_0>: n=331456, size=2176, socket=0
testpmd: preferred mempool ops selected: ring_mp_mc
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: create a new mbuf pool <mbuf_pool_socket_1>: n=331456, size=2176,
socket=1
testpmd: preferred mempool ops selected: ring_mp_mc
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory)
----

[id="cnf-best-practices-cnf-network-security"]
== Securing CNF networks

CNFs must have the least permissions possible and CNFs must implement Network Policies that drop all traffic by default and permit only the relevant ports and protocols to the narrowest ranges of addresses possible.

.CNF requirement
[IMPORTANT]
====
Applications must define network policies that permit only the minimum network access the application needs to function.
====

[id="cnf-best-practices-secrets-management"]
== Managing secrets

Secrets objects in OpenShift provide a way to hold sensitive information such as passwords, config files and credentials. There are 4 types of secrets; service account, basic auth, ssh auth and TLS. Secrets can be added via deployment configurations or consumed by pods directly. For more information on secrets and examples, see the following documentation.

link:https://docs.openshift.com/container-platform/latest/nodes/pods/nodes-pods-secrets.html[Providing sensitive data to pods]

[id="cnf-best-practices-scc-permissions-for-an-application"]
== Setting SCC permissions for applications

Permissions to use an SCC is done by adding a cluster role that has _uses_ permissions for the SCC and then rolebindings for the users within a namespace to that role for users that need that SCC. Application admins can create their own role/rolebindings to assign permissions to a Service Account.


