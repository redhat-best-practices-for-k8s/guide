= Requests and Limits in Kubernetes

Kubernetes provides mechanisms for defining resource usage per container:

* *Requests*: The guaranteed minimum amount of a resource (e.g., CPU, memory). Used by the scheduler.
* *Limits*: The maximum amount a container is allowed to consume. Enforced by the kubelet.
* *Quotas*: Enforce aggregate resource usage at the namespace/project level to prevent resource overuse.

See: link:https://docs.openshift.com/container-platform/latest/nodes/clusters/nodes-cluster-resource-configure.html#nodes-cluster-resource-configure-resource-quotas_nodes-cluster-resource-configure[OpenShift Resource Quotas Per Project]

*Risks with Resource Limits*

While limits can prevent runaway resource usage, they also introduce risk when misapplied, especially for CPU and memory.

CPU Limits Cause Throttling

* Limits can throttle workloads even if unused CPU is available.
* This leads to hangs, timeouts, and degraded performance.
* CPU requests (without limits) often provide better fairness and stability.

Memory Limits Cause OOMKills

* Limits on memory are strictâ€”when exceeded, containers are killed.
* Difficult to predict worst-case memory usage for infrastructure components.
* Can result in crash loops, degraded service, and unrecoverable clusters.

*Why Limits are a Problem for Cluster Components*

Unlike with user workloads, setting resource limits for cluster components presents several challenges and is strongly discouraged:

* *Inability to Anticipate Scaling*: Cluster components cannot predict their usage scaling across all customer environments, making it impossible to set one-size-fits-all limits.
* *Impeded Responsiveness*: Setting static limits prevents administrators from reacting to changes in cluster needs, such as resizing control plane nodes to allocate more resources.
* *Undesirable Restarts*: It is undesirable for cluster components to be restarted due to excessive resource consumption (e.g., OOMKills). Graceful handling without degrading cluster performance is preferred.

Therefore, *cluster components SHOULD NOT be configured with resource limits*.

However, *cluster components MUST declare resource requests for both CPU and memory*.

*Benefits of Using Requests Without Limits*

* *Guaranteed Minimums and Bursting*: Specifying requests without limits ensures components receive their required minimum resources and can burst when needed.
* *Balancing Efficiency and Performance*: When setting resource requests:
  ** If too low, the component may be starved under load, leading to degraded performance and service.
  ** If too high, the scheduler may be unable to place the component, leading to crash loops or failed deployments. Excessively high requests can also starve user workloads, particularly in small or single-node clusters.

*Resource Requests: Compressible vs Incompressible*

Kubernetes treats resources differently depending on their behavior under pressure:

[cols="1,2,2", options="header"]
|===
|Resource Type |Description |Examples
|Compressible |Slower performance but still runs |CPU, network
|Incompressible |Fails without required amount |Memory, storage
|===

*Requesting Resources*

* *Compressible (e.g., CPU)*: Requests should be balanced to ensure proportional system behavior and fairness.
* *Incompressible (e.g., memory)*: Requests should reflect minimum safe usage to avoid runtime failure.

See: link:https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[More details on setting requests for different resource types]

*Alternatives to Resource Limits*

Although limits are generally avoided for cluster components, the following mechanisms can help manage resources and prioritize workloads:

* *Pod Priority (PriorityClass)*: Preferred for ensuring essential core workloads have priority and sufficient resources.
  ** Allows critical components to avoid eviction during resource pressure.
  ** More effective than limits, as kubelet's eviction does not consider limits.
* *Quotas*: Can enforce requests and limits at the project level.
  ** Useful when guaranteed capacity is needed.
  ** Overcommitment trade-offs can be managed at the project, node, or cluster level.

*Overcommitment*

Nodes can be overcommitted, influencing how requests and limits are applied.

* Overcommitted clusters often experience pod kills and kubelet restarts, degrading the product experience and increasing support costs.
* Use quotas to enforce guaranteed capacity in production.
* In development, overcommitment may be acceptable where performance trade-offs are tolerable.

See: link:https://docs.openshift.com/container-platform/latest/nodes/scheduling/nodes-scheduler-overcommit.html[Configuring your cluster to place pods on overcommitted nodes]

See test case link:https://github.com/redhat-best-practices-for-k8s/certsuite/blob/main/CATALOG.md#access-control-requests-and-limits[access-control-requests-and-limits]
